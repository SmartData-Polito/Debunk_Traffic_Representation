{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset\n",
    "\n",
    "This is for Split data to three datasets (train, val, test)\n",
    "\n",
    "All flows are disjointly, the test distribution is same with origin datasets\n",
    "\n",
    "Train and Test datasets are balanced, which means every class has similar number packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scapy.all as scapy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import json\n",
    "\n",
    "os.chdir('LLM4Traffic/tool/Split-Dataset')\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(       \n",
    "    level=logging.INFO,            \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  \n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/split_based_flow.log', mode='w'),  \n",
    "        logging.StreamHandler()          \n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 3 # k-folder\n",
    "seed = 43 # random seed\n",
    "threshold = 5 # threshold for the number of packets in a flow\n",
    "test_size = 1/(k+1) # test size for train-test split\n",
    "random.seed(seed)\n",
    "\n",
    "dataset = 'vpn-app'\n",
    "dataset_path = 'external_pcaps/ISCX-VPN-2016/Filtered/App/sessions'\n",
    "output_path = 'external_pcaps/ISCX-VPN-2016/Filtered/App/flow'\n",
    "\n",
    "if dataset == 'vpn-app':\n",
    "    datasets_class_name = ['aim', 'email', 'facebook', 'sftp', 'gmail', 'hangout', 'icq', 'netflix', 'scp', 'ftp', 'skype', 'spotify', 'vimeo', 'torrent', 'voipbuster', 'youtube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# statistics the information of the dataset\n",
    "# num_of_class = len(datasets_class_name)\n",
    "num_of_original_flow = 0\n",
    "num_of_filtered_flow = 0\n",
    "num_of_flow_per_class = defaultdict(int) # number of flows per class, which is larger than threshold\n",
    "flow_file_of_class = defaultdict(list) # list of flow files per class\n",
    "\n",
    "def process_flow(args):\n",
    "    flow, folder_path, class_name = args\n",
    "    count = 0;\n",
    "    with scapy.PcapReader(f\"{folder_path}/{flow}\") as packets:\n",
    "        for _ in packets:\n",
    "            count += 1\n",
    "            if count >= threshold:\n",
    "                return class_name, f\"{folder_path}/{flow}\"\n",
    "        \n",
    "for folder in os.listdir(dataset_path): # AIM_chat_1 directory\n",
    "    logger.info(f\"Processing folder: {folder}\")\n",
    "    if dataset == 'tls' or dataset == 'vpn-app':\n",
    "        class_name = folder\n",
    "    else:\n",
    "        class_name = next((name for name in datasets_class_name if name in folder or name.upper() in folder), None)\n",
    "        if class_name is None:\n",
    "            logger.warning(f\"No matching class for folder: {folder}\")\n",
    "            continue\n",
    "    logger.info(f\"Class name: {class_name}\")\n",
    "\n",
    "    flows = [flow for flow in os.listdir(f\"{dataset_path}/{folder}\")] # all flow\n",
    "    num_of_original_flow += len(flows)\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.map(process_flow, [(flow, f\"{dataset_path}/{folder}\", class_name) for flow in flows])\n",
    "\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            class_name, flow = result\n",
    "            num_of_filtered_flow += 1\n",
    "            num_of_flow_per_class[class_name] += 1\n",
    "            flow_file_of_class[class_name].append(flow)\n",
    "\n",
    "results = {\n",
    "    'num_of_original_flow': num_of_original_flow,\n",
    "    'num_of_filtered_flow': num_of_filtered_flow,\n",
    "    'num_of_flow_per_class': dict(num_of_flow_per_class),\n",
    "    'flow_file_of_class': dict(flow_file_of_class)\n",
    "}\n",
    "\n",
    "with open(f'statistics/{dataset}_{threshold}_flow.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "logger.info('Results saved to results.json')\n",
    "logger.info('Data processing completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_num_of_flow_per_class = {key: int(test_size * value) for key, value in num_of_flow_per_class.items()}\n",
    "\n",
    "train_val_num_of_flow_per_class = {key: value - test_num_of_flow_per_class[key] for key, value in num_of_flow_per_class.items()}\n",
    "\n",
    "if dataset == 'tls': # because the latest class is too small, we use the second smallest class\n",
    "    sorted_values = sorted(train_val_num_of_flow_per_class.values())\n",
    "    min_train_num_of_flow_per_class= sorted_values[1]\n",
    "else:\n",
    "    min_train_num_of_flow_per_class = min(train_val_num_of_flow_per_class.values())   \n",
    "\n",
    "train_val_num_of_flow_per_class = {key: min(min_train_num_of_flow_per_class, value) for key, value in train_val_num_of_flow_per_class.items()}\n",
    "\n",
    "print(test_num_of_flow_per_class)\n",
    "print(train_val_num_of_flow_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the test and train-val set\n",
    "results = {}\n",
    "train_val_flow_file_of_class = defaultdict(list)\n",
    "test_flow_file_of_class = defaultdict(list)\n",
    "\n",
    "for class_name, flow_files in flow_file_of_class.items():\n",
    "    test_flow_file_of_class[class_name] = random.sample(flow_files, test_num_of_flow_per_class[class_name])\n",
    "\n",
    "for class_name, flow_files in flow_file_of_class.items():\n",
    "    train_val_flow_file_of_class[class_name] = list(set(flow_files) - set(test_flow_file_of_class[class_name]))\n",
    "\n",
    "k_folds = defaultdict(list)\n",
    "for class_name, flow_files in train_val_flow_file_of_class.items():\n",
    "    random.shuffle(flow_files)\n",
    "    fold_size = train_val_num_of_flow_per_class[class_name] // k\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        k_folds[class_name].append(flow_files[start:end])\n",
    "\n",
    "for i in range(k):\n",
    "    results[f'k_{i}'] = {class_name: k_folds[class_name][i] for class_name in k_folds}\n",
    "results['test'] = test_flow_file_of_class\n",
    "\n",
    "with open(f'outputs/{dataset}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the output directory of the test dataset\n",
    "os.makedirs(f\"{output_path}/test\", exist_ok=True)\n",
    "\n",
    "for class_name, flow_files in test_flow_file_of_class.items():\n",
    "    logger.info(f\"Copying test files of class {class_name}\")\n",
    "    os.makedirs(f\"{output_path}/test/{class_name}\", exist_ok=True)\n",
    "    for flow_file in flow_files:\n",
    "        command_result = os.system(f\"cp {flow_file} {output_path}/test/{class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the output directory of the train-val dataset\n",
    "for i in range(k):\n",
    "    os.makedirs(f\"{output_path}/train_val_split_{i}\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_path}/train_val_split_{i}/val\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_path}/train_val_split_{i}/train\", exist_ok=True)\n",
    "\n",
    "    for class_name, flow_files in k_folds.items():\n",
    "        logger.info(f\"Copying val files {i} of class {class_name} in split {i}\")\n",
    "        os.makedirs(f\"{output_path}/train_val_split_{i}/val/{class_name}\", exist_ok=True)\n",
    "\n",
    "        for flow_file in flow_files[i]:\n",
    "            command_result = os.system(f\"cp {flow_file} {output_path}/train_val_split_{i}/val/{class_name}\")\n",
    "\n",
    "\n",
    "    other_index = list(range(i)) + list(range(i+1, k))\n",
    "    for j in other_index:\n",
    "        for class_name, flow_files in k_folds.items():\n",
    "            logger.info(f\"Copying train files {j} of class {class_name} in split {i}\")\n",
    "            os.makedirs(f\"{output_path}/train_val_split_{i}/train/{class_name}\", exist_ok=True)\n",
    "\n",
    "            for flow_file in flow_files[j]:\n",
    "                command_result = os.system(f\"cp {flow_file} {output_path}/train_val_split_{i}/train/{class_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_process]",
   "language": "python",
   "name": "conda-env-data_process-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
