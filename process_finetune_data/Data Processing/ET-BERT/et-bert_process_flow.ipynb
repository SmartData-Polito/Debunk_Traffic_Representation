{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "process et-bert data for fine-tuning\n",
    "label.pcap -> train/validation/test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scapy.all as scapy\n",
    "import random\n",
    "import binascii\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "os.chdir('LLM4Traffic/tool/Data-Process')\n",
    "\n",
    "logging.basicConfig(       \n",
    "    level=logging.INFO,            \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  \n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/et-bert_process_flow.log', mode='w'),  \n",
    "        logging.StreamHandler()          \n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(obj, sec):\n",
    "    result = [obj[i:i+sec] for i in range(0,len(obj),sec)]\n",
    "    try:\n",
    "        remanent_count = len(result[0])%4\n",
    "    except Exception as e:\n",
    "        remanent_count = 0\n",
    "        print(\"cut datagram error!\")\n",
    "    if remanent_count == 0:\n",
    "        pass\n",
    "    else:\n",
    "        result = [obj[i:i+sec+remanent_count] for i in range(0,len(obj),sec+remanent_count)]\n",
    "    return result\n",
    "\n",
    "def bigram_generation(packet_datagram, packet_len = 64, flag=True):\n",
    "    result = ''\n",
    "    generated_datagram = cut(packet_datagram,1)\n",
    "    token_count = 0\n",
    "    for sub_string_index in range(len(generated_datagram)):\n",
    "        if sub_string_index != (len(generated_datagram) - 1):\n",
    "            token_count += 1\n",
    "            if token_count > packet_len:\n",
    "                break\n",
    "            else:\n",
    "                merge_word_bigram = generated_datagram[sub_string_index] + generated_datagram[sub_string_index + 1]\n",
    "        else:\n",
    "            break\n",
    "        result += merge_word_bigram\n",
    "        result += ' '\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = '/share/smartdata/external_pcaps/ISCX-VPN-2016/Filtered/App/flow'\n",
    "# output_path = f'LLM4Traffic/code/ET-BERT/data_flow/vpn-app'\n",
    "\n",
    "dataset_path = '/share/smartdata/external_pcaps/CSTNET-TLS1.3/Filtered/flow'\n",
    "output_path = f'LLM4Traffic/code/ET-BERT/data_flow/tls'\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for packet-level\n",
    "# type: train, test, val\n",
    "# file: pcap file\n",
    "# this is for generating dataset for ET-BERT with pcap and parquet files\n",
    "# dataset_path/{train_val/test}/class_name/flow.pcap\n",
    "# dataset_path/{train_val}/{train/val}/class_name/flow.pcap\n",
    "\n",
    "def clean_packet(packet):\n",
    "    if packet.haslayer(scapy.Ether):\n",
    "        packet = packet[scapy.Ether].payload\n",
    "\n",
    "    if packet.haslayer(scapy.IP):\n",
    "        packet = packet[scapy.IP].payload\n",
    "    elif packet.haslayer('IPv6'):\n",
    "        packet = packet['IPv6'].payload\n",
    "\n",
    "    if packet.haslayer(scapy.UDP):\n",
    "        packet[scapy.UDP].sport = 0  \n",
    "        packet[scapy.UDP].dport = 0  \n",
    "    elif packet.haslayer(scapy.TCP):\n",
    "        packet[scapy.TCP].sport = 0  \n",
    "        packet[scapy.TCP].dport = 0  \n",
    "    \n",
    "    return packet\n",
    "\n",
    "def get_feature_packet(packet, payload_length):\n",
    "    packet_data_string = ''\n",
    "    packet_data = packet.copy()\n",
    "    packet_string = (binascii.hexlify(bytes(packet_data))).decode()[8:]  # remove eth header, ip header and port\n",
    "    packet_data_string += bigram_generation(packet_string, packet_len=payload_length, flag=True)\n",
    "    return packet_data_string\n",
    "\n",
    "def save_to_tsv(dataset_file, output_path, type):\n",
    "    with open(f\"{output_path}/{type}.tsv\", 'w', newline='') as f:\n",
    "        tsv_w = csv.writer(f, delimiter='\\t')\n",
    "        tsv_w.writerows(dataset_file)\n",
    "\n",
    "def process_file(path, class_name, payload_length):\n",
    "    dataset_file = [[\"label\", \"text_a\"]]\n",
    "    dataset_numpy = []\n",
    "    dataset_label = []\n",
    "\n",
    "    pkts = scapy.PcapReader(f\"{dataset_path}/{path}.pcap\")\n",
    "\n",
    "    feature_packet = ''\n",
    "    for id, pkt in enumerate(pkts):\n",
    "        if id < 5:\n",
    "            pkt = clean_packet(pkt)\n",
    "            feature_packet += get_feature_packet(pkt, payload_length)\n",
    "        else:\n",
    "            break\n",
    "    dataset_file.append([int(class_name), feature_packet])\n",
    "    dataset_numpy.append(feature_packet)\n",
    "    dataset_label.append(class_name)\n",
    "    # logger.info(f\"Finish processing, the length of dataset is {len(dataset_numpy)}\")\n",
    "    return dataset_file, dataset_numpy, dataset_label\n",
    "\n",
    "def generate_dataset(dataset_path, output_path, payload_length):\n",
    "    for type in os.listdir(f'{dataset_path}'):\n",
    "        logger.info(f\"Start processing {dataset_path}/{type}\")\n",
    "\n",
    "        if type == 'test':\n",
    "            dataset_file_list, dataset_numpy_list, dataset_label_list = [], [], []\n",
    "            class_id = 0\n",
    "            for class_name in os.listdir(f'{dataset_path}/{type}'):\n",
    "                for flow_id, file_name in enumerate(os.listdir(f'{dataset_path}/{type}/{class_name}')):\n",
    "                    logger.info(f\"Start processing {type}/{class_name}/{file_name}\")\n",
    "\n",
    "                    dataset_file, dataset_numpy, dataset_label = process_file(f\"{type}/{class_name}/{file_name[:-5]}\", class_id, payload_length)\n",
    "\n",
    "                    if class_id == 0 and flow_id == 0:\n",
    "                        dataset_file_list.extend(dataset_file)\n",
    "                    else:\n",
    "                        dataset_file_list.extend(dataset_file[1:])\n",
    "\n",
    "                    dataset_numpy_list.extend(dataset_numpy)\n",
    "                    dataset_label_list.extend(dataset_label)\n",
    "\n",
    "                class_id += 1\n",
    "            print(class_id)\n",
    "\n",
    "            save_to_tsv(dataset_file_list, output_path, type)\n",
    "            np.save(f\"{output_path}/x_payload_{type}.npy\", dataset_numpy_list)\n",
    "            np.save(f\"{output_path}/y_label_{type}.npy\", dataset_label_list)\n",
    "        else:\n",
    "            for folder in os.listdir(f'{dataset_path}/{type}'):\n",
    "                logger.info(f\"Start processing {dataset_path}/{type}/{folder}\")\n",
    "                \n",
    "                dataset_file_list, dataset_numpy_list, dataset_label_list = [], [], []\n",
    "                class_id = 0\n",
    "                for class_name in os.listdir(f'{dataset_path}/{type}/{folder}'):\n",
    "                    for flow_id, file_name in enumerate(os.listdir(f'{dataset_path}/{type}/{folder}/{class_name}')):\n",
    "                        logger.info(f\"Start processing {dataset_path}/{type}/{folder}/{class_name}/{file_name}\")\n",
    "\n",
    "                        dataset_file, dataset_numpy, dataset_label = process_file(f\"{type}/{folder}/{class_name}/{file_name[:-5]}\", class_id, payload_length)\n",
    "\n",
    "                        if class_id == 0 and flow_id == 0:\n",
    "                            dataset_file_list.extend(dataset_file)\n",
    "                        else:\n",
    "                            dataset_file_list.extend(dataset_file[1:])\n",
    "\n",
    "                        dataset_numpy_list.extend(dataset_numpy)\n",
    "                        dataset_label_list.extend(dataset_label)\n",
    "\n",
    "                    class_id += 1\n",
    "                \n",
    "                print(class_id)\n",
    "                save_to_tsv(dataset_file_list, f\"{output_path}/{type}\", folder)\n",
    "                np.save(f\"{output_path}/{type}/x_payload_{folder}.npy\", dataset_numpy_list)\n",
    "                np.save(f\"{output_path}/{type}/y_label_{folder}.npy\", dataset_label_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset/type(train, test, validation)/.pcap\n",
    "generate_dataset(dataset_path, output_path, payload_length = 128)\n",
    "\n",
    "logger.info(f'Finish')\n",
    "# main(dataset_path='your_dataset_path', type='your_type', output_path='your_output_path', payload_length=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_process]",
   "language": "python",
   "name": "conda-env-data_process-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
