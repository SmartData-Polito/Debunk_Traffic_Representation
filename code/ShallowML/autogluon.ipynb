{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon for automatic task\n",
    "Try to use Autogluon for traditional baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "os.chdir('LLM4Traffic/code/Traditional')\n",
    "\n",
    "logging.basicConfig(       \n",
    "    level=logging.INFO,            \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  \n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/AutoGluon.log', mode='w'),  \n",
    "        logging.StreamHandler()          \n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'tls'\n",
    "type = 'add_info_ip'\n",
    "experiment = 'polishednsLen811'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: /home/dauin_user/yzhao/LLM4Traffic/code/Traditional/datasets/polishednsLen811/tls/add_info_ip/train_val_split_0/train.csv | Columns = 31 / 31 | Rows = 486516 -> 486516\n",
      "Loaded data from: /home/dauin_user/yzhao/LLM4Traffic/code/Traditional/datasets/polishednsLen811/tls/add_info_ip/train_val_split_0/val.csv | Columns = 31 / 31 | Rows = 60816 -> 60816\n",
      "Loaded data from: /home/dauin_user/yzhao/LLM4Traffic/code/Traditional/datasets/polishednsLen811/tls/add_info_ip/test.csv | Columns = 31 / 31 | Rows = 60871 -> 60871\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutogluonModels/polishednsLen811/tls/add_info_ip_0_123\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.11.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #173-Ubuntu SMP PREEMPT Tue Jul 11 08:08:57 UTC 2023\n",
      "CPU Count:          64\n",
      "Memory Avail:       326.10 GB / 376.41 GB (86.6%)\n",
      "Disk Space Avail:   925335.94 GB / 1374003.67 GB (67.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (486516 samples, 150.4 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/polishednsLen811/tls/add_info_ip_0_123\"\n",
      "Train Data Rows:    486516\n",
      "Train Data Columns: 30\n",
      "Tuning Data Rows:    60816\n",
      "Tuning Data Columns: 30\n",
      "Label Column:       class\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 120) unique label values:  ['51cto.pcap', 'sina.pcap', 'overleaf.pcap', 'youtube.pcap', 'alibaba.pcap', 'instagram.pcap', 'iqiyi.pcap', 'ibm.pcap', 'dailymotion.pcap', 'smzdm.pcap']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 120\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    334048.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 125.27 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 6): ['TCP_reserved', 'IP_version', 'IP_proto', 'IP_frag', 'TCP_urgptr', 'IP_ihl']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 11 | ['IP_id', 'TCP_Timestamp', 'IP_tos', 'TCP_ack', 'TCP_SAck', ...]\n",
      "\t\t('int', [])   : 13 | ['TCP_dataofs', 'IP_ttl', 'TL_chksum', 'IP_dst_0', 'IP_src_1', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 11 | ['IP_id', 'TCP_Timestamp', 'IP_tos', 'TCP_ack', 'TCP_SAck', ...]\n",
      "\t\t('int', [])       : 12 | ['TCP_dataofs', 'IP_ttl', 'TL_chksum', 'IP_dst_0', 'IP_src_1', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['IP_flags']\n",
      "\t1.1s = Fit runtime\n",
      "\t24 features in original data used to generate 24 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 96.57 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'NN_TORCH': {},\n",
      "\t'XGB': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t0.9616\t = Validation score   (accuracy)\n",
      "\t33.06s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9892\t = Validation score   (accuracy)\n",
      "\t22.56s\t = Training   runtime\n",
      "\t4.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9899\t = Validation score   (accuracy)\n",
      "\t50.27s\t = Training   runtime\n",
      "\t4.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "/home/dauin_user/yzhao/conda_envs/autogluon/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [11:19:45] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1733179782501/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/dauin_user/yzhao/conda_envs/autogluon/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [11:24:58] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1733179782501/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t0.995\t = Validation score   (accuracy)\n",
      "\t313.53s\t = Training   runtime\n",
      "\t2.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "/home/dauin_user/yzhao/conda_envs/autogluon/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dauin_user/yzhao/conda_envs/autogluon/lib/python3.11/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t0.9425\t = Validation score   (accuracy)\n",
      "\t1634.35s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 1.0}\n",
      "\t0.995\t = Validation score   (accuracy)\n",
      "\t13.88s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2103.93s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 25903.4 rows/s (60816 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/polishednsLen811/tls/add_info_ip_0_123\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "data_path = f'LLM4Traffic/code/Traditional/datasets/{experiment}/{dataset}/{type}'\n",
    "\n",
    "for id in range(1):\n",
    "    train_data = TabularDataset(f'{data_path}/train_val_split_{id}/train.csv')\n",
    "    tuning_data = TabularDataset(f'{data_path}/train_val_split_{id}/val.csv')\n",
    "    test_data = TabularDataset(f'{data_path}/test.csv')\n",
    "    model_path = f'AutogluonModels/{experiment}/{dataset}/{type}_{id}_123'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    predictor = TabularPredictor(label='class', path=model_path).fit(train_data = train_data, tuning_data = tuning_data, num_gpus = 1, \n",
    "                                                    hyperparameters = {\n",
    "                                                        'GBM': [{}],\n",
    "                                                        'NN_TORCH': {},\n",
    "                                                        'XGB': {},\n",
    "                                                        'RF': [\n",
    "                                                            {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
    "                                                            {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, \n",
    "                                                            {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}]\n",
    "                                                    })\n",
    "\n",
    "    os.makedirs(f'results/{experiment}/{dataset}/{type}', exist_ok=True)\n",
    "    predictor.leaderboard(test_data, extra_metrics=['f1_macro', 'f1_micro', 'precision_macro', 'precision_micro', 'recall_macro', 'recall_micro']).to_csv(f'results/{experiment}/{dataset}/{type}/{dataset}_{id}_123.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestGini\n",
      "Reported Acc Result: 81.8 ± 3.0 (95% confidence level)\n",
      "Reported F1-Macro Result: 78.0 ± 3.0 (95% confidence level)\n",
      "Reported F1-micro Result: 81.8 ± 3.0 (95% confidence level)\n",
      "Reported Time Result: 41.0 ± 8.9 (95% confidence level)\n",
      "\n",
      "XGBoost\n",
      "Reported Acc Result: 85.1 ± 1.9 (95% confidence level)\n",
      "Reported F1-Macro Result: 82.0 ± 1.7 (95% confidence level)\n",
      "Reported F1-micro Result: 85.1 ± 1.9 (95% confidence level)\n",
      "Reported Time Result: 207.9 ± 106.7 (95% confidence level)\n",
      "\n",
      "LightGBM\n",
      "Reported Acc Result: 85.6 ± 1.8 (95% confidence level)\n",
      "Reported F1-Macro Result: 82.4 ± 2.0 (95% confidence level)\n",
      "Reported F1-micro Result: 85.6 ± 1.8 (95% confidence level)\n",
      "Reported Time Result: 544.5 ± 425.1 (95% confidence level)\n",
      "\n",
      "NeuralNetTorch\n",
      "Reported Acc Result: 74.1 ± 2.8 (95% confidence level)\n",
      "Reported F1-Macro Result: 68.8 ± 2.9 (95% confidence level)\n",
      "Reported F1-micro Result: 74.1 ± 2.8 (95% confidence level)\n",
      "Reported Time Result: 1.7 ± 0.1 (95% confidence level)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "def calculate_ci(data):\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data, ddof=1)\n",
    "    sem = std_dev / np.sqrt(len(data))\n",
    "    confidence_interval = stats.t.interval(0.95, len(data)-1, loc=mean, scale=sem)\n",
    "\n",
    "    measurement_result = mean\n",
    "    margin_of_error = (confidence_interval[1] - confidence_interval[0]) / 2\n",
    "\n",
    "    return measurement_result * 100, margin_of_error * 100\n",
    "\n",
    "models = ['RandomForestGini', 'XGBoost', 'LightGBM', 'NeuralNetTorch']\n",
    "acc = {model: [] for model in models}  # 初始化字典，模型名为键，值为空列表\n",
    "f1_macro = {model: [] for model in models}   # 同上\n",
    "f1_micro = {model: [] for model in models}   # 同上\n",
    "inference_time = {model: [] for model in models}\n",
    "\n",
    "# 遍历不同的id\n",
    "for id in range(3):\n",
    "    # 读取当前id的数据\n",
    "    df = pd.read_csv(f'results/{experiment}/{dataset}/{type}/{dataset}_{id}.csv')\n",
    "    # 遍历每个模型，直接从数据中筛选出对应模型的score_test和f1_macro\n",
    "    for model in models:\n",
    "        model_data = df[df['model'] == model]\n",
    "        acc[model].append(model_data['score_test'].values[0])  # 假设每个模型对应的score_test只有一个值\n",
    "        f1_macro[model].append(model_data['f1_macro'].values[0])  # 同理，f1_macro也只有一个值\n",
    "        f1_micro[model].append(model_data['f1_micro'].values[0])\n",
    "        inference_time[model].append(model_data['pred_time_test'].values[0])\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "\n",
    "    measurement_result, margin_of_error = calculate_ci(acc[model])\n",
    "    print(f\"Reported Acc Result: {measurement_result:.1f} ± {margin_of_error:.1f} (95% confidence level)\")\n",
    "\n",
    "    measurement_result, margin_of_error = calculate_ci(f1_macro[model])\n",
    "    print(f\"Reported F1-Macro Result: {measurement_result:.1f} ± {margin_of_error:.1f} (95% confidence level)\")\n",
    "\n",
    "    measurement_result, margin_of_error = calculate_ci(f1_micro[model])\n",
    "    print(f\"Reported F1-micro Result: {measurement_result:.1f} ± {margin_of_error:.1f} (95% confidence level)\")\n",
    "    \n",
    "    measurement_result, margin_of_error = calculate_ci(inference_time[model])\n",
    "    print(f\"Reported Time Result: {measurement_result/100:.1f} ± {margin_of_error/100:.1f} (95% confidence level)\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:autogluon]",
   "language": "python",
   "name": "conda-env-autogluon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
