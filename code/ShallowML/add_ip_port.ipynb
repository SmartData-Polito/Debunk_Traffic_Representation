{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "process pcap data for get traditional baseline\n",
    "label.pcap -> feature alignment Matrix/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scapy.all as scapy\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('/home/dauin_user/yzhao/debunk_representation/code/Traditional')\n",
    "\n",
    "logging.basicConfig(       \n",
    "    level=logging.INFO,            \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  \n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/add_ip_port.log', mode='w'),  \n",
    "        logging.StreamHandler()          \n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '/home/dauin_user/yzhao/explanation/data/per-packet/tls/ood'\n",
    "output_path = '/home/dauin_user/yzhao/debunk_representation/code/Traditional/datasets/polishednsLen811/tls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_key_val(line):\n",
    "    layer = None\n",
    "    sublayer = None\n",
    "    key = None\n",
    "    val = None\n",
    "\n",
    "    if '###' in line:             \n",
    "        if '|###' in line:                \n",
    "            sublayer = line.strip('|#[] ')  \n",
    "            print(f'sublayer ### {sublayer}')    \n",
    "        else:                \n",
    "            layer = line.strip('#[] ')      \n",
    "\n",
    "    if '=' in line:            \n",
    "        if '|' in line and 'sublayer' in locals():                \n",
    "            key, val = line.strip('| ').split('=', 1)        \n",
    "            key, val = key.strip(), val.strip('\\' ')          \n",
    "            print(f'sublayer ||| {sublayer}')  \n",
    "        else:                 \n",
    "            key, val = line.split('=', 1)   \n",
    "            key, val = key.strip(), val.strip('\\' ')    \n",
    "            \n",
    "    return layer, sublayer, key, val\n",
    "\n",
    "def clean_packet(packet):\n",
    "    if packet.haslayer(scapy.Ether):\n",
    "        packet = packet[scapy.Ether].payload\n",
    "\n",
    "    if packet.haslayer(scapy.IP) or packet.haslayer('IPv6'):\n",
    "        if packet.haslayer(scapy.UDP):\n",
    "            packet[scapy.UDP].remove_payload()   \n",
    "        if packet.haslayer(scapy.TCP):\n",
    "            packet[scapy.TCP].remove_payload()    \n",
    "    \n",
    "    return packet\n",
    "\n",
    "# get how many features/ip flags/tcp flags/.. in the dataset\n",
    "\n",
    "pass_fields = [] # ignore these fields\n",
    "field_set = set()\n",
    "ip_flags_set = set()\n",
    "tcp_flags_set = set()\n",
    "\n",
    "def get_all_features(packet):\n",
    "    protocol = None\n",
    "    for line in packet.show2(dump=True).split('\\n'): \n",
    "        layer, sub, key, val = get_key_val(line)\n",
    "\n",
    "        if layer is not None:\n",
    "                protocol = layer\n",
    "        else:\n",
    "            if key not in pass_fields and key is not None:   \n",
    "                if key == 'options': # options is a list, there are two new fields\n",
    "                    if len(val) == 0:\n",
    "                        continue\n",
    "                    for option in eval(val):\n",
    "                            if option[0] == 'NOP': # ignore NOP (No Operation)\n",
    "                                continue\n",
    "                            elif option[0] == 'Timestamp': # Timestamp and Echo are a pair\n",
    "                                field_set.add(f'{protocol}_{option[0]}')\n",
    "                                field_set.add(f'{protocol}_echo_{option[0]}')\n",
    "                            elif option[0] == 'SAck': # SAckOK is a flag\n",
    "                                field_set.add(f'{protocol}_{option[0]}')\n",
    "                                field_set.add(f'{protocol}_echo_{option[0]}')\n",
    "                                continue\n",
    "\n",
    "                            field_set.add(f'{protocol}_{option[0]}')\n",
    "                    continue     \n",
    "\n",
    "                if protocol == 'IP':\n",
    "                    if key == 'src' or key == 'dst':\n",
    "                        for i in range(4):\n",
    "                            field_set.add(f'{protocol}_{key}_{i}')\n",
    "                        continue\n",
    "                elif protocol == 'IPv6':\n",
    "                    if key == 'src' or key == 'dst':\n",
    "                        for i in range(8):\n",
    "                            field_set.add(f'{protocol}_{key}_{i}')\n",
    "                        continue\n",
    "\n",
    "                # distinguish ip flags and tcp flags\n",
    "                if key == 'flags': \n",
    "                    if protocol == 'IP':\n",
    "                        ip_flags_set.add(f'{val}')\n",
    "                    elif protocol == 'TCP':\n",
    "                        tcp_flags_set.add(f'{val}')  \n",
    "\n",
    "                if key == 'chksum' and protocol in ['TCP', 'UDP']:   # merge tcp and udp checksum\n",
    "                    field_set.add('TL_chksum')\n",
    "                if key == 'sport' or key == 'dport': # merge sport and dport\n",
    "                    if protocol in ['TCP', 'UDP']:\n",
    "                        field_set.add(f'TL_{key}')\n",
    "                elif f'{protocol}_{key}'== 'IP_tos' or f'{protocol}_{key}' == 'IPv6_tc': # merge tos and traffic class\n",
    "                    field_set.add('IP_tos')\n",
    "                elif key == 'version': # merge ip version and ipv6 version\n",
    "                    field_set.add('IP_version')\n",
    "                elif f'{protocol}_{key}'== 'IP_ttl' or f'{protocol}_{key}' == 'IPv6_hlim': # merge ttl and hot limit\n",
    "                    field_set.add('IP_ttl')\n",
    "                elif f'{protocol}_{key}'== 'IP_len' or f'{protocol}_{key}' == 'IPv6_plen': # merge len and payload len\n",
    "                    field_set.add('IP_len')\n",
    "                elif f'{protocol}_{key}'== 'IP_proto' or f'{protocol}_{key}' == 'IPv6_nh': # merge proto and next header\n",
    "                    field_set.add('IP_proto')\n",
    "                else:\n",
    "                    field_set.add(f'{protocol}_{key}')\n",
    "\n",
    "\n",
    "                \n",
    "    return field_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:25:34,753 - root - INFO - Processing test\n",
      "2025-04-17 13:25:34,754 - root - INFO - Processing test 51cto.pcap\n",
      "2025-04-17 13:25:35,370 - root - INFO - Processing test sina.pcap\n",
      "2025-04-17 13:25:36,055 - root - INFO - Processing test overleaf.pcap\n",
      "2025-04-17 13:25:38,205 - root - INFO - Processing test youtube.pcap\n",
      "2025-04-17 13:25:38,740 - root - INFO - Processing test alibaba.pcap\n",
      "2025-04-17 13:25:38,973 - root - INFO - Processing test instagram.pcap\n",
      "2025-04-17 13:25:39,395 - root - INFO - Processing test iqiyi.pcap\n",
      "2025-04-17 13:25:39,808 - root - INFO - Processing test ibm.pcap\n",
      "2025-04-17 13:25:40,246 - root - INFO - Processing test dailymotion.pcap\n",
      "2025-04-17 13:25:41,943 - root - INFO - Processing test smzdm.pcap\n",
      "2025-04-17 13:25:43,144 - root - INFO - Processing test gravatar.pcap\n",
      "2025-04-17 13:25:43,547 - root - INFO - Processing test microsoft.pcap\n",
      "2025-04-17 13:25:44,050 - root - INFO - Processing test cloudfront.pcap\n",
      "2025-04-17 13:25:44,331 - root - INFO - Processing test jb51.pcap\n",
      "2025-04-17 13:25:45,531 - root - INFO - Processing test pinduoduo.pcap\n",
      "2025-04-17 13:25:46,168 - root - INFO - Processing test vmware.pcap\n",
      "2025-04-17 13:25:46,573 - root - INFO - Processing test wikimedia.pcap\n",
      "2025-04-17 13:25:47,584 - root - INFO - Processing test sohu.pcap\n",
      "2025-04-17 13:25:48,008 - root - INFO - Processing test apple.pcap\n",
      "2025-04-17 13:25:48,421 - root - INFO - Processing test twimg.pcap\n",
      "2025-04-17 13:25:49,803 - root - INFO - Processing test onlinedown.pcap\n",
      "2025-04-17 13:25:50,581 - root - INFO - Processing test booking.pcap\n",
      "2025-04-17 13:25:50,936 - root - INFO - Processing test digitaloceanspaces.pcap\n",
      "2025-04-17 13:25:51,216 - root - INFO - Processing test netflix.pcap\n",
      "2025-04-17 13:25:51,721 - root - INFO - Processing test gitlab.pcap\n",
      "2025-04-17 13:25:52,481 - root - INFO - Processing test criteo.pcap\n",
      "2025-04-17 13:25:52,987 - root - INFO - Processing test v2ex.pcap\n",
      "2025-04-17 13:25:53,768 - root - INFO - Processing test icloud.pcap\n",
      "2025-04-17 13:25:54,224 - root - INFO - Processing test yahoo.pcap\n",
      "2025-04-17 13:25:54,617 - root - INFO - Processing test runoob.pcap\n",
      "2025-04-17 13:25:55,176 - root - INFO - Processing test sciencedirect.pcap\n",
      "2025-04-17 13:25:55,454 - root - INFO - Processing test huya.pcap\n",
      "2025-04-17 13:25:55,931 - root - INFO - Processing test xiaomi.pcap\n",
      "2025-04-17 13:25:56,765 - root - INFO - Processing test huanqiu.pcap\n",
      "2025-04-17 13:25:57,146 - root - INFO - Processing test eastmoney.pcap\n",
      "2025-04-17 13:25:57,542 - root - INFO - Processing test elsevier.pcap\n",
      "2025-04-17 13:25:57,952 - root - INFO - Processing test office.pcap\n",
      "2025-04-17 13:25:58,664 - root - INFO - Processing test 51.pcap\n",
      "2025-04-17 13:25:58,949 - root - INFO - Processing test semanticscholar.pcap\n",
      "2025-04-17 13:25:59,581 - root - INFO - Processing test kugou.pcap\n",
      "2025-04-17 13:26:00,100 - root - INFO - Processing test ximalaya.pcap\n",
      "2025-04-17 13:26:00,385 - root - INFO - Processing test naver.pcap\n",
      "2025-04-17 13:26:01,071 - root - INFO - Processing test cisco.pcap\n",
      "2025-04-17 13:26:01,364 - root - INFO - Processing test mi.pcap\n",
      "2025-04-17 13:26:01,778 - root - INFO - Processing test leetcode-cn.pcap\n",
      "2025-04-17 13:26:02,874 - root - INFO - Processing test arxiv.pcap\n",
      "2025-04-17 13:26:03,446 - root - INFO - Processing test mozilla.pcap\n",
      "2025-04-17 13:26:03,920 - root - INFO - Processing test walmart.pcap\n",
      "2025-04-17 13:26:04,180 - root - INFO - Processing test feishu.pcap\n",
      "2025-04-17 13:26:04,691 - root - INFO - Processing test wikipedia.pcap\n",
      "2025-04-17 13:26:05,611 - root - INFO - Processing test vk.pcap\n",
      "2025-04-17 13:26:05,996 - root - INFO - Processing test spring.pcap\n",
      "2025-04-17 13:26:06,394 - root - INFO - Processing test notion.pcap\n",
      "2025-04-17 13:26:06,931 - root - INFO - Processing test grammarly.pcap\n",
      "2025-04-17 13:26:07,316 - root - INFO - Processing test nvidia.pcap\n",
      "2025-04-17 13:26:08,129 - root - INFO - Processing test taboola.pcap\n",
      "2025-04-17 13:26:09,154 - root - INFO - Processing test yy.pcap\n",
      "2025-04-17 13:26:09,897 - root - INFO - Processing test atlassian.pcap\n",
      "2025-04-17 13:26:10,182 - root - INFO - Processing test toutiao.pcap\n",
      "2025-04-17 13:26:10,501 - root - INFO - Processing test amap.pcap\n",
      "2025-04-17 13:26:10,680 - root - INFO - Processing test qq.pcap\n",
      "2025-04-17 13:26:11,156 - root - INFO - Processing test bilibili.pcap\n",
      "2025-04-17 13:26:11,641 - root - INFO - Processing test acm.pcap\n",
      "2025-04-17 13:26:13,341 - root - INFO - Processing test adobe.pcap\n",
      "2025-04-17 13:26:13,818 - root - INFO - Processing test google.pcap\n",
      "2025-04-17 13:26:14,236 - root - INFO - Processing test chinatax.pcap\n",
      "2025-04-17 13:26:14,619 - root - INFO - Processing test baidu.pcap\n",
      "2025-04-17 13:26:15,101 - root - INFO - Processing test paypal.pcap\n",
      "2025-04-17 13:26:15,399 - root - INFO - Processing test yandex.pcap\n",
      "2025-04-17 13:26:16,055 - root - INFO - Processing test oracle.pcap\n",
      "2025-04-17 13:26:16,683 - root - INFO - Processing test ctrip.pcap\n",
      "2025-04-17 13:26:17,263 - root - INFO - Processing test codepen.pcap\n",
      "2025-04-17 13:26:17,680 - root - INFO - Processing test cnblogs.pcap\n",
      "2025-04-17 13:26:18,565 - root - INFO - Processing test steampowered.pcap\n",
      "2025-04-17 13:26:18,933 - root - INFO - Processing test weibo.pcap\n",
      "2025-04-17 13:26:20,463 - root - INFO - Processing test squarespace.pcap\n",
      "2025-04-17 13:26:20,777 - root - INFO - Processing test zhihu.pcap\n",
      "2025-04-17 13:26:21,550 - root - INFO - Processing test jd.pcap\n",
      "2025-04-17 13:26:21,915 - root - INFO - Processing test t.pcap\n",
      "2025-04-17 13:26:22,224 - root - INFO - Processing test biligame.pcap\n",
      "2025-04-17 13:26:22,590 - root - INFO - Processing test eastday.pcap\n",
      "2025-04-17 13:26:22,934 - root - INFO - Processing test qcloud.pcap\n",
      "2025-04-17 13:26:23,169 - root - INFO - Processing test alipay.pcap\n",
      "2025-04-17 13:26:23,535 - root - INFO - Processing test goat.pcap\n",
      "2025-04-17 13:26:24,135 - root - INFO - Processing test guancha.pcap\n",
      "2025-04-17 13:26:24,617 - root - INFO - Processing test gmail.pcap\n",
      "2025-04-17 13:26:24,955 - root - INFO - Processing test python.pcap\n",
      "2025-04-17 13:26:25,206 - root - INFO - Processing test chia.pcap\n",
      "2025-04-17 13:26:25,503 - root - INFO - Processing test springer.pcap\n",
      "2025-04-17 13:26:26,172 - root - INFO - Processing test github.pcap\n",
      "2025-04-17 13:26:26,570 - root - INFO - Processing test 163.pcap\n",
      "2025-04-17 13:26:27,079 - root - INFO - Processing test crazyegg.pcap\n",
      "2025-04-17 13:26:27,410 - root - INFO - Processing test thepaper.pcap\n",
      "2025-04-17 13:26:28,121 - root - INFO - Processing test ampproject.pcap\n",
      "2025-04-17 13:26:28,728 - root - INFO - Processing test vivo.pcap\n",
      "2025-04-17 13:26:29,157 - root - INFO - Processing test tiktok.pcap\n",
      "2025-04-17 13:26:29,418 - root - INFO - Processing test wp.pcap\n",
      "2025-04-17 13:26:29,966 - root - INFO - Processing test researchgate.pcap\n",
      "2025-04-17 13:26:31,119 - root - INFO - Processing test asus.pcap\n",
      "2025-04-17 13:26:31,376 - root - INFO - Processing test snapchat.pcap\n",
      "2025-04-17 13:26:31,744 - root - INFO - Processing test huawei.pcap\n",
      "2025-04-17 13:26:32,925 - root - INFO - Processing test ggpht.pcap\n",
      "2025-04-17 13:26:33,984 - root - INFO - Processing test duckduckgo.pcap\n",
      "2025-04-17 13:26:34,359 - root - INFO - Processing test unity3d.pcap\n",
      "2025-04-17 13:26:34,726 - root - INFO - Processing test deepl.pcap\n",
      "2025-04-17 13:26:35,862 - root - INFO - Processing test nike.pcap\n",
      "2025-04-17 13:26:37,028 - root - INFO - Processing test alicdn.pcap\n",
      "2025-04-17 13:26:38,136 - root - INFO - Processing test opera.pcap\n",
      "2025-04-17 13:26:38,424 - root - INFO - Processing test cloudflare.pcap\n",
      "2025-04-17 13:26:38,826 - root - INFO - Processing test twitter.pcap\n",
      "2025-04-17 13:26:39,446 - root - INFO - Processing test facebook.pcap\n",
      "2025-04-17 13:26:40,031 - root - INFO - Processing test statcounter.pcap\n",
      "2025-04-17 13:26:40,341 - root - INFO - Processing test azureedge.pcap\n",
      "2025-04-17 13:26:40,789 - root - INFO - Processing test msn.pcap\n",
      "2025-04-17 13:26:41,105 - root - INFO - Processing test media.pcap\n",
      "2025-04-17 13:26:41,679 - root - INFO - Processing test amazonaws.pcap\n",
      "2025-04-17 13:26:42,042 - root - INFO - Processing test hubspot.pcap\n",
      "2025-04-17 13:26:42,527 - root - INFO - Processing test ieee.pcap\n",
      "2025-04-17 13:26:42,846 - root - INFO - Processing test outbrain.pcap\n",
      "2025-04-17 13:26:43,233 - root - INFO - Processing test teads.pcap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IP_proto', 'IP_chksum', 'TCP_echo_SAck', 'IP_len', 'IP_frag', 'IP_src_3', 'IP_src_1', 'IP_dst_2', 'IP_version', 'IP_flags', 'IP_id', 'IP_dst_0', 'TCP_echo_Timestamp', 'TL_chksum', 'IP_dst_1', 'TCP_SAck', 'IP_dst_3', 'TCP_reserved', 'IP_src_2', 'TL_dport', 'IP_ihl', 'TCP_urgptr', 'TCP_ack', 'TCP_Timestamp', 'TCP_chksum', 'IP_ttl', 'TCP_dataofs', 'TCP_flags', 'IP_src_0', 'IP_tos', 'TL_sport', 'TCP_seq', 'TCP_window'}\n",
      "{'', 'DF'}\n",
      "{'FPA', 'PA', 'A'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# file/test/{test.pcap} or file/train_val_split_0/{train/val}/.pcap\n",
    "for type in os.listdir(dataset_path):\n",
    "    logger.info(f'Processing {type}')\n",
    "\n",
    "    if type == 'test':\n",
    "        for class_id, file_name in enumerate(os.listdir(f'{dataset_path}/{type}')):\n",
    "            if 'pcap' in file_name:\n",
    "                logger.info(f'Processing {type} {file_name}')\n",
    "\n",
    "                packets =  scapy.PcapReader(f'{dataset_path}/{type}/{file_name}')\n",
    "                for id, packet in enumerate(packets):\n",
    "                    # only require ip header and transportaion layer\n",
    "                    packet = clean_packet(packet)\n",
    "                    get_all_features(packet)\n",
    "                \n",
    "    else:\n",
    "        for class_id, folder in enumerate(os.listdir(f'{dataset_path}/{type}')):\n",
    "            logger.info(f'Processing {type} {folder}')\n",
    "            \n",
    "            for file_name in os.listdir(f'{dataset_path}/{type}/{folder}'):\n",
    "                logger.info(f'Processing {type} {folder} {file_name}')\n",
    "                if 'pcap' in file_name:\n",
    "                    packets =  scapy.PcapReader(f'{dataset_path}/{type}/{folder}/{file_name}')\n",
    "                    for id, packet in enumerate(packets):\n",
    "                        \n",
    "                        # only require ip header and transportaion layer\n",
    "                        packet = clean_packet(packet)\n",
    "                        get_all_features(packet)\n",
    "                                            \n",
    "print(field_set)\n",
    "print(ip_flags_set)\n",
    "print(tcp_flags_set)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# convert the flag to integer\n",
    "\n",
    "ip_flags_map = {}\n",
    "tcp_flags_map = {}\n",
    "\n",
    "for id, ip_flag in enumerate(ip_flags_set):\n",
    "    if ip_flag != '':\n",
    "        ip_flags_map[ip_flag] = id + 1\n",
    "\n",
    "for id, tcp_flag in enumerate(tcp_flags_set):\n",
    "    if tcp_flag != '':\n",
    "        tcp_flags_map[tcp_flag] = id + 1\n",
    "\n",
    "print(ip_flags_map)\n",
    "print(tcp_flags_map)\n",
    "print(field_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_set = {'IP_src_2', 'TCP_chksum', 'TCP_echo_SAck', 'IP_src_0', 'TL_chksum', 'TL_sport', 'IP_ihl', 'IP_dst_1', 'IP_dst_2', 'IP_frag', 'TCP_ack', 'IP_len', 'TCP_window', 'TCP_flags', 'IP_src_1', 'IP_chksum', 'IP_flags', 'TCP_SAck', 'IP_src_3', 'TCP_echo_Timestamp', 'TCP_urgptr', 'IP_dst_3', 'IP_dst_0', 'IP_proto', 'IP_id', 'IP_tos', 'IP_ttl', 'TCP_reserved', 'TCP_seq', 'TL_dport', 'TCP_dataofs', 'TCP_Timestamp', 'IP_version'}\n",
    "ip_flags_map = {'DF': 1}\n",
    "tcp_flags_map = {'FA': 1, 'FPAC': 2, 'FPA': 3, 'A': 4, 'PA': 5, 'PAC': 6, 'AC': 7}\n",
    "ip_proto_map = {'tcp': 1, 'udp': 2, 'TCP': 1, 'UDP': 2}\n",
    "port_map = {'https': 443, 'supfiledbg': 1127, 'remctl': 4373, 'ospf6d': 2606, 'daap': 3689, 'svn': 3690, 'radius': 1812, 'dircproxy': 57000, 'gnutella_rtr': 6347, 'imaps': 993, 'fido': 60179, 'git': 9418, 'gsiftp': 2811, 'nfs': 2049, 'gnunet': 2086, 'x11_2': 6002, 'redis': 6379, 'radmin_port': 4899, 'amqp': 5672, 'cfengine': 5308, 'venus': 2430, 'iprop': 2121, 'x11_3': 6003, 'tfido': 60177, 'x11_1': 6000, 'x11_6': 6006, 'fax': 4557}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass_fields = ['src', 'dst', 'sport', 'dport']\n",
    "pass_fields = []\n",
    "port_fields = set()\n",
    "map_features = {'TCP_flags', 'IP_flags', 'IP_proto'}\n",
    "hex_features = {'IP_tos', 'IP_chksum', 'TL_chksum', 'UDP_chksum', 'TCP_chksum'}\n",
    "port_features = {'TL_sport', 'TL_dport'}\n",
    "equal_features = {'TCP_chksum', 'UDP_chksum', 'IPV6_tc', 'IPv6_version', 'IPv6_plen', 'IPv6_nh', 'IPv6_hlim', 'UDP_sport', 'UDP_dport', 'TCP_sport', 'TCP_dport'}\n",
    "\n",
    "def hex2int(value):\n",
    "    return int(value, 16)\n",
    "\n",
    "def special_rules(field, value):\n",
    "    if not value: # convert '' to 0\n",
    "        return 0\n",
    "    elif field == 'TCP_flags':     # convert flags to int\n",
    "        return tcp_flags_map[value]\n",
    "    elif field == 'IP_flags':     # convert flags to int\n",
    "        return ip_flags_map[value]\n",
    "    elif field == 'IP_proto': # convert protocol to int\n",
    "        return ip_proto_map[value]\n",
    "    \n",
    "def equal_rules(field): # convert ipv6 fields name to ipv4 fields name\n",
    "    if field == 'TCP_chksum' or field == 'UDP_chksum':\n",
    "        return 'TL_chksum'\n",
    "    elif field == 'IPv6_tc':\n",
    "        return 'IP_tos'\n",
    "    elif field == 'IPv6_version':\n",
    "        return 'IP_version'\n",
    "    elif field == 'IPv6_plen':\n",
    "        return 'IP_len'\n",
    "    elif field == 'IPv6_nh':\n",
    "        return 'IP_proto'\n",
    "    elif field == 'IPv6_hlim':\n",
    "        return 'IP_ttl'\n",
    "    elif field == 'UDP_sport' or field == 'TCP_sport':\n",
    "        return 'TL_sport'\n",
    "    elif field == 'UDP_dport' or field == 'TCP_dport':\n",
    "        return 'TL_dport'\n",
    "     \n",
    "def packet2features(packet):\n",
    "    protocol = None\n",
    "    packet_key_val = {}\n",
    "    for line in packet.show2(dump=True).split('\\n'): \n",
    "        layer, _, key, val = get_key_val(line)\n",
    "        if layer is not None:\n",
    "                protocol = layer\n",
    "        else:\n",
    "            if key not in pass_fields and key is not None:   \n",
    "                if key == 'options': # options is a list, there are two new fields\n",
    "                    for option in eval(val):\n",
    "                            if option[0] == 'NOP': # ignore NOP (No Operation)\n",
    "                                continue\n",
    "                            elif option[0] == 'Timestamp': # Timestamp and Echo are a pair, Echo is the second field\n",
    "                                packet_key_val[f'{protocol}_{option[0]}'] = option[1][0]\n",
    "                                packet_key_val[f'{protocol}_echo_{option[0]}'] = option[1][1]\n",
    "                            elif option[0] == 'SAck': # The format of SAck is smiliar with timestamp and they are a pair, Echo is the second field\n",
    "                                packet_key_val[f'{protocol}_{option[0]}'] = option[1][0]\n",
    "                                packet_key_val[f'{protocol}_echo_{option[0]}'] = option[1][1]\n",
    "                            elif option[0] == 'SAckOK': \n",
    "                                packet_key_val[f'{protocol}_{option[0]}'] = int(val == 1)\n",
    "                            elif option[0] == 'EOL': # EOL is a flag, always 0\n",
    "                                packet_key_val[f'{protocol}_{option[0]}'] = 0\n",
    "                            else:\n",
    "                                packet_key_val[f'{protocol}_{option[0]}'] = option[1]\n",
    "                    continue   \n",
    "\n",
    "                if protocol == 'IP':\n",
    "                    if key == 'src' or key == 'dst':\n",
    "                        for i, ip in enumerate(val.split('.')):\n",
    "                            packet_key_val[f'{protocol}_{key}_{i}'] = int(ip)\n",
    "                        continue\n",
    "                elif protocol == 'IPv6':\n",
    "                    if key == 'src' or key == 'dst':\n",
    "                        ipv6_split = val.split(':')\n",
    "                        for i, ip in enumerate(ipv6_split):\n",
    "                            if ip != '':\n",
    "                                packet_key_val[f'{protocol}_{key}_{i}'] = int(ip, 16)\n",
    "                            else:\n",
    "                                for _ in range(8 - len(ipv6_split)):\n",
    "                                    packet_key_val[f'{protocol}_{key}_{i}'] = 0\n",
    "                        continue\n",
    "\n",
    "                if f'{protocol}_{key}' in equal_features: # special rules for some fields\n",
    "                    field = equal_rules(f'{protocol}_{key}')\n",
    "                else:\n",
    "                    field = f'{protocol}_{key}'\n",
    "\n",
    "\n",
    "                if field in map_features: # special rules for some fields\n",
    "                    packet_key_val[field] = special_rules(field, val)\n",
    "                elif field in hex_features:\n",
    "                    packet_key_val[field] = hex2int(val)\n",
    "                elif field in port_features:\n",
    "                    if val in port_map:\n",
    "                        packet_key_val[field] = port_map[val]\n",
    "                    else:\n",
    "                        packet_key_val[field] = val\n",
    "\n",
    "                else:\n",
    "                    packet_key_val[field] = int(val) \n",
    "                \n",
    "    packet_array = [packet_key_val.get(key, 0) for key in field_set]\n",
    "\n",
    "    return packet_array\n",
    "\n",
    "def write2csv(type, data, label):\n",
    "    os.makedirs(f'{output_path}/unclean_ood_test', exist_ok=True)\n",
    "    file_path = f'{output_path}/unclean_ood_test/{type}.csv'\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        with open(file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            print(list(field_set) + ['class'])\n",
    "            writer.writerow(list(field_set) + ['class'])\n",
    "\n",
    "    with open(file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(data + [label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 15:37:36,783 - root - INFO - Processing 3-tuple-test\n",
      "2025-04-29 15:37:36,784 - root - INFO - Processing test\n",
      "2025-04-29 15:37:36,809 - root - INFO - Processing test 51cto.pcap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP_chksum', 'IP_flags', 'IP_ttl', 'IP_src_0', 'TCP_urgptr', 'TCP_ack', 'TCP_Timestamp', 'IP_id', 'TL_chksum', 'TCP_seq', 'TCP_dataofs', 'IP_src_3', 'IP_dst_1', 'TCP_window', 'IP_dst_2', 'TCP_chksum', 'TL_dport', 'TCP_flags', 'IP_dst_3', 'IP_version', 'TL_sport', 'IP_src_2', 'IP_tos', 'IP_dst_0', 'IP_frag', 'IP_len', 'TCP_reserved', 'TCP_echo_Timestamp', 'IP_ihl', 'TCP_SAck', 'IP_proto', 'TCP_echo_SAck', 'IP_src_1', 'class']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 15:37:38,734 - root - INFO - Processing test sina.pcap\n",
      "2025-04-29 15:37:39,160 - root - INFO - Processing test overleaf.pcap\n",
      "2025-04-29 15:37:40,658 - root - INFO - Processing test youtube.pcap\n",
      "2025-04-29 15:37:41,031 - root - INFO - Processing test alibaba.pcap\n",
      "2025-04-29 15:37:41,184 - root - INFO - Processing test instagram.pcap\n",
      "2025-04-29 15:37:41,458 - root - INFO - Processing test iqiyi.pcap\n",
      "2025-04-29 15:37:41,759 - root - INFO - Processing test ibm.pcap\n",
      "2025-04-29 15:37:42,089 - root - INFO - Processing test dailymotion.pcap\n",
      "2025-04-29 15:37:43,359 - root - INFO - Processing test smzdm.pcap\n",
      "2025-04-29 15:37:44,257 - root - INFO - Processing test gravatar.pcap\n",
      "2025-04-29 15:37:44,541 - root - INFO - Processing test microsoft.pcap\n",
      "2025-04-29 15:37:44,906 - root - INFO - Processing test cloudfront.pcap\n",
      "2025-04-29 15:37:45,115 - root - INFO - Processing test jb51.pcap\n",
      "2025-04-29 15:37:45,862 - root - INFO - Processing test pinduoduo.pcap\n",
      "2025-04-29 15:37:46,231 - root - INFO - Processing test vmware.pcap\n",
      "2025-04-29 15:37:46,490 - root - INFO - Processing test wikimedia.pcap\n",
      "2025-04-29 15:37:47,135 - root - INFO - Processing test sohu.pcap\n",
      "2025-04-29 15:37:47,398 - root - INFO - Processing test apple.pcap\n",
      "2025-04-29 15:37:47,655 - root - INFO - Processing test twimg.pcap\n",
      "2025-04-29 15:37:48,517 - root - INFO - Processing test onlinedown.pcap\n",
      "2025-04-29 15:37:49,007 - root - INFO - Processing test booking.pcap\n",
      "2025-04-29 15:37:49,212 - root - INFO - Processing test digitaloceanspaces.pcap\n",
      "2025-04-29 15:37:49,386 - root - INFO - Processing test netflix.pcap\n",
      "2025-04-29 15:37:49,755 - root - INFO - Processing test gitlab.pcap\n",
      "2025-04-29 15:37:50,293 - root - INFO - Processing test criteo.pcap\n",
      "2025-04-29 15:37:50,610 - root - INFO - Processing test v2ex.pcap\n",
      "2025-04-29 15:37:51,087 - root - INFO - Processing test icloud.pcap\n",
      "2025-04-29 15:37:51,379 - root - INFO - Processing test yahoo.pcap\n",
      "2025-04-29 15:37:51,630 - root - INFO - Processing test runoob.pcap\n",
      "2025-04-29 15:37:52,053 - root - INFO - Processing test sciencedirect.pcap\n",
      "2025-04-29 15:37:52,252 - root - INFO - Processing test huya.pcap\n",
      "2025-04-29 15:37:52,610 - root - INFO - Processing test xiaomi.pcap\n",
      "2025-04-29 15:37:53,162 - root - INFO - Processing test huanqiu.pcap\n",
      "2025-04-29 15:37:53,397 - root - INFO - Processing test eastmoney.pcap\n",
      "2025-04-29 15:37:53,639 - root - INFO - Processing test elsevier.pcap\n",
      "2025-04-29 15:37:53,885 - root - INFO - Processing test office.pcap\n",
      "2025-04-29 15:37:54,337 - root - INFO - Processing test 51.pcap\n",
      "2025-04-29 15:37:54,512 - root - INFO - Processing test semanticscholar.pcap\n",
      "2025-04-29 15:37:54,959 - root - INFO - Processing test kugou.pcap\n",
      "2025-04-29 15:37:55,314 - root - INFO - Processing test ximalaya.pcap\n",
      "2025-04-29 15:37:55,505 - root - INFO - Processing test naver.pcap\n",
      "2025-04-29 15:37:55,934 - root - INFO - Processing test cisco.pcap\n",
      "2025-04-29 15:37:56,117 - root - INFO - Processing test mi.pcap\n",
      "2025-04-29 15:37:56,410 - root - INFO - Processing test leetcode-cn.pcap\n",
      "2025-04-29 15:37:57,266 - root - INFO - Processing test arxiv.pcap\n",
      "2025-04-29 15:37:57,617 - root - INFO - Processing test mozilla.pcap\n",
      "2025-04-29 15:37:57,942 - root - INFO - Processing test walmart.pcap\n",
      "2025-04-29 15:37:58,095 - root - INFO - Processing test feishu.pcap\n",
      "2025-04-29 15:37:58,461 - root - INFO - Processing test wikipedia.pcap\n",
      "2025-04-29 15:37:59,085 - root - INFO - Processing test vk.pcap\n",
      "2025-04-29 15:37:59,351 - root - INFO - Processing test spring.pcap\n",
      "2025-04-29 15:37:59,606 - root - INFO - Processing test notion.pcap\n",
      "2025-04-29 15:37:59,954 - root - INFO - Processing test grammarly.pcap\n",
      "2025-04-29 15:38:00,228 - root - INFO - Processing test nvidia.pcap\n",
      "2025-04-29 15:38:00,784 - root - INFO - Processing test taboola.pcap\n",
      "2025-04-29 15:38:01,429 - root - INFO - Processing test yy.pcap\n",
      "2025-04-29 15:38:01,872 - root - INFO - Processing test atlassian.pcap\n",
      "2025-04-29 15:38:02,044 - root - INFO - Processing test toutiao.pcap\n",
      "2025-04-29 15:38:02,266 - root - INFO - Processing test amap.pcap\n",
      "2025-04-29 15:38:02,395 - root - INFO - Processing test qq.pcap\n",
      "2025-04-29 15:38:02,695 - root - INFO - Processing test bilibili.pcap\n",
      "2025-04-29 15:38:02,983 - root - INFO - Processing test acm.pcap\n",
      "2025-04-29 15:38:04,045 - root - INFO - Processing test adobe.pcap\n",
      "2025-04-29 15:38:04,367 - root - INFO - Processing test google.pcap\n",
      "2025-04-29 15:38:04,640 - root - INFO - Processing test chinatax.pcap\n",
      "2025-04-29 15:38:04,871 - root - INFO - Processing test baidu.pcap\n",
      "2025-04-29 15:38:05,175 - root - INFO - Processing test paypal.pcap\n",
      "2025-04-29 15:38:05,354 - root - INFO - Processing test yandex.pcap\n",
      "2025-04-29 15:38:05,757 - root - INFO - Processing test oracle.pcap\n",
      "2025-04-29 15:38:06,166 - root - INFO - Processing test ctrip.pcap\n",
      "2025-04-29 15:38:06,563 - root - INFO - Processing test codepen.pcap\n",
      "2025-04-29 15:38:06,862 - root - INFO - Processing test cnblogs.pcap\n",
      "2025-04-29 15:38:07,478 - root - INFO - Processing test steampowered.pcap\n",
      "2025-04-29 15:38:07,752 - root - INFO - Processing test weibo.pcap\n",
      "2025-04-29 15:38:08,822 - root - INFO - Processing test squarespace.pcap\n",
      "2025-04-29 15:38:09,034 - root - INFO - Processing test zhihu.pcap\n",
      "2025-04-29 15:38:09,587 - root - INFO - Processing test jd.pcap\n",
      "2025-04-29 15:38:09,843 - root - INFO - Processing test t.pcap\n",
      "2025-04-29 15:38:10,027 - root - INFO - Processing test biligame.pcap\n",
      "2025-04-29 15:38:10,257 - root - INFO - Processing test eastday.pcap\n",
      "2025-04-29 15:38:10,473 - root - INFO - Processing test qcloud.pcap\n",
      "2025-04-29 15:38:10,643 - root - INFO - Processing test alipay.pcap\n",
      "2025-04-29 15:38:10,918 - root - INFO - Processing test goat.pcap\n",
      "2025-04-29 15:38:11,380 - root - INFO - Processing test guancha.pcap\n",
      "2025-04-29 15:38:11,728 - root - INFO - Processing test gmail.pcap\n",
      "2025-04-29 15:38:11,967 - root - INFO - Processing test python.pcap\n",
      "2025-04-29 15:38:12,135 - root - INFO - Processing test chia.pcap\n",
      "2025-04-29 15:38:12,326 - root - INFO - Processing test springer.pcap\n",
      "2025-04-29 15:38:12,775 - root - INFO - Processing test github.pcap\n",
      "2025-04-29 15:38:13,026 - root - INFO - Processing test 163.pcap\n",
      "2025-04-29 15:38:13,385 - root - INFO - Processing test crazyegg.pcap\n",
      "2025-04-29 15:38:13,594 - root - INFO - Processing test thepaper.pcap\n",
      "2025-04-29 15:38:14,068 - root - INFO - Processing test ampproject.pcap\n",
      "2025-04-29 15:38:14,514 - root - INFO - Processing test vivo.pcap\n",
      "2025-04-29 15:38:14,780 - root - INFO - Processing test tiktok.pcap\n",
      "2025-04-29 15:38:14,946 - root - INFO - Processing test wp.pcap\n",
      "2025-04-29 15:38:15,310 - root - INFO - Processing test researchgate.pcap\n",
      "2025-04-29 15:38:16,115 - root - INFO - Processing test asus.pcap\n",
      "2025-04-29 15:38:16,294 - root - INFO - Processing test snapchat.pcap\n",
      "2025-04-29 15:38:16,559 - root - INFO - Processing test huawei.pcap\n",
      "2025-04-29 15:38:17,389 - root - INFO - Processing test ggpht.pcap\n",
      "2025-04-29 15:38:18,172 - root - INFO - Processing test duckduckgo.pcap\n",
      "2025-04-29 15:38:18,394 - root - INFO - Processing test unity3d.pcap\n",
      "2025-04-29 15:38:18,611 - root - INFO - Processing test deepl.pcap\n",
      "2025-04-29 15:38:19,309 - root - INFO - Processing test nike.pcap\n",
      "2025-04-29 15:38:20,042 - root - INFO - Processing test alicdn.pcap\n",
      "2025-04-29 15:38:20,701 - root - INFO - Processing test opera.pcap\n",
      "2025-04-29 15:38:20,869 - root - INFO - Processing test cloudflare.pcap\n",
      "2025-04-29 15:38:21,120 - root - INFO - Processing test twitter.pcap\n",
      "2025-04-29 15:38:21,506 - root - INFO - Processing test facebook.pcap\n",
      "2025-04-29 15:38:21,869 - root - INFO - Processing test statcounter.pcap\n",
      "2025-04-29 15:38:22,061 - root - INFO - Processing test azureedge.pcap\n",
      "2025-04-29 15:38:22,333 - root - INFO - Processing test msn.pcap\n",
      "2025-04-29 15:38:22,540 - root - INFO - Processing test media.pcap\n",
      "2025-04-29 15:38:22,888 - root - INFO - Processing test amazonaws.pcap\n",
      "2025-04-29 15:38:23,131 - root - INFO - Processing test hubspot.pcap\n",
      "2025-04-29 15:38:23,424 - root - INFO - Processing test ieee.pcap\n",
      "2025-04-29 15:38:23,625 - root - INFO - Processing test outbrain.pcap\n",
      "2025-04-29 15:38:23,814 - root - INFO - Processing test teads.pcap\n",
      "2025-04-29 15:38:24,058 - root - INFO - Processing 5-tuple-test\n",
      "2025-04-29 15:38:24,060 - root - INFO - Finished\n"
     ]
    }
   ],
   "source": [
    "# file/test/{test.pcap} or file/train_val_split_0/{train/val}/.pcap\n",
    "\n",
    "for type in os.listdir(dataset_path):\n",
    "    logger.info(f'Processing {type}')\n",
    "\n",
    "    if 'test' == type:\n",
    "        for class_id, file_name in enumerate(os.listdir(f'{dataset_path}/{type}')):\n",
    "            if 'pcap' in file_name:\n",
    "                logger.info(f'Processing {type} {file_name}')\n",
    "\n",
    "                packets =  scapy.PcapReader(f'{dataset_path}/{type}/{file_name}')\n",
    "                for id, packet in enumerate(packets):\n",
    "                    packet = clean_packet(packet)\n",
    "                    packet_array = packet2features(packet)\n",
    "                    write2csv(type, packet_array, file_name)\n",
    "    elif 'train_val_split' in type:\n",
    "        for class_id, folder in enumerate(os.listdir(f'{dataset_path}/{type}')):\n",
    "            logger.info(f'Processing {type} {folder}')\n",
    "\n",
    "            for file_name in os.listdir(f'{dataset_path}/{type}/{folder}'):\n",
    "                if 'pcap' in file_name:\n",
    "                    packets =  scapy.PcapReader(f'{dataset_path}/{type}/{folder}/{file_name}')\n",
    "                    for id, packet in enumerate(packets):\n",
    "                        packet = clean_packet(packet)\n",
    "                        packet_array = packet2features(packet)\n",
    "                        write2csv(f'{type}/{folder}', packet_array, file_name)\n",
    " \n",
    "logger.info('Finished')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for data clean\n",
    "\n",
    "normalization each row\n",
    "\n",
    "**train and test should use same distribution of normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For USTCTFC2016\n",
    "field_set = {'TCP_window', 'TCP_chksum', 'TCP_reserved', 'TCP_ack', 'TCP_echo_SAck', 'TCP_flags', 'TCP_SAckOK', 'IP_id', 'IP_proto', 'IP_len', 'TCP_seq', 'TCP_WScale', 'TCP_EOL', 'UDP_chksum', 'UDP_len', 'TCP_dataofs', 'IP_version', 'TCP_SAck', 'TCP_MSS', 'IP_chksum', 'IP_flags', 'TCP_urgptr', 'IP_frag', 'IP_ttl', 'TCP_echo_Timestamp', 'TCP_Timestamp', 'IP_ihl', 'IP_tos'}\n",
    "\n",
    "ip_flags_map = {'DF': 1}\n",
    "tcp_flags_map = {'PA': 1, 'S': 2, 'FA': 3, 'A': 4, 'FPA': 5, 'R': 6, 'SA': 7, 'RA': 8}\n",
    "ip_proto_map = {'tcp': 1, 'udp': 2}\n",
    "\n",
    "normalize_set = {'TCP_window', 'TCP_chksum', 'TCP_ack', 'TCP_echo_SAck', 'IP_id', 'IP_len', 'TCP_seq', 'UDP_chksum', 'UDP_len','TCP_SAck', 'TCP_MSS', 'IP_chksum', 'TCP_echo_Timestamp', 'TCP_Timestamp', 'IP_tos'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val_split_0 train.csv\n",
      "train_val_split_0 val.csv\n",
      "train_val_split_1 train.csv\n",
      "train_val_split_1 val.csv\n",
      "train_val_split_2 train.csv\n",
      "train_val_split_2 val.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "normalize_set = {'TCP_window', 'TCP_chksum', 'TCP_ack', 'TCP_echo_SAck', 'IP_id', 'IP_len', 'TCP_seq', 'UDP_chksum', 'UDP_len','TCP_SAck', 'TCP_MSS', 'IP_chksum', 'TCP_echo_Timestamp', 'TCP_Timestamp', 'IP_tos'}\n",
    "types = ['train_val_split_0', 'train_val_split_1', 'train_val_split_2', 'test.csv']\n",
    "\n",
    "def log_normalize(value):\n",
    "    return np.log1p(value)\n",
    "\n",
    "for type in types:\n",
    "    if type == 'test.csv':\n",
    "        df = pd.read_csv(f'{output_path}/unclean_add_ip_port/{type}')\n",
    "        for field in normalize_set:\n",
    "            if field in df.columns:\n",
    "                df[field] = df[field].apply(log_normalize)\n",
    "        df.to_csv(f'{output_path}/add_info/{type}', index=False)\n",
    "    else:\n",
    "        for file in os.listdir(f'{output_path}/unclean_add_ip_port/{type}'):\n",
    "            df = pd.read_csv(f'{output_path}/unclean_add_ip_port/{type}/{file}')\n",
    "            print(type, file)\n",
    "            for field in normalize_set:\n",
    "                if field in df.columns:\n",
    "                    df[field] = df[field].apply(log_normalize)\n",
    "            df.to_csv(f'{output_path}/add_info/{type}/{file}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For TLS\n",
    "field_set = {'TCP_window', 'TCP_chksum', 'TCP_reserved', 'TCP_ack', 'TCP_echo_SAck', 'TCP_flags', 'IP_id', 'IP_proto', 'IP_len', 'TCP_seq', 'TCP_dataofs', 'IP_version', 'TCP_SAck', 'IP_chksum', 'TCP_urgptr', 'IP_flags', 'IP_frag', 'IP_ttl', 'TCP_echo_Timestamp', 'TCP_Timestamp', 'IP_ihl', 'IP_tos'}\n",
    "\n",
    "ip_flags_map = {'DF': 1}\n",
    "tcp_flags_map = {'PA': 1, 'FA': 2, 'A': 3, 'FPA': 4, 'R': 5, 'PAC': 6, 'RA': 7}\n",
    "ip_proto_map = {'tcp': 1, 'udp': 2}\n",
    "\n",
    "normalize_set = {'TCP_window', 'TCP_chksum', 'TCP_ack', 'TCP_echo_SAck', 'IP_id', 'IP_len', 'TCP_seq', 'TCP_SAck', 'IP_chksum','TCP_echo_Timestamp', 'TCP_Timestamp', 'IP_tos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "normalize_set = {'TCP_window', 'TCP_chksum', 'TCP_ack', 'TCP_echo_SAck', 'IP_id', 'IP_len', 'TCP_seq', 'TCP_SAck', 'IP_chksum','TCP_echo_Timestamp', 'TCP_Timestamp', 'IP_tos'}\n",
    "types = ['test.csv']\n",
    "\n",
    "def log_normalize(value):\n",
    "    return np.log1p(value)\n",
    "\n",
    "for type in types:\n",
    "    if type == 'test.csv':\n",
    "        df = pd.read_csv(f'{output_path}/unclean_ood_test/{type}')\n",
    "        if 'TCP_30' in df.columns:\n",
    "            print('OK')\n",
    "            df = df.drop(columns = ['TCP_30'])\n",
    "        for field in normalize_set:\n",
    "            if field in df.columns:\n",
    "                df[field] = df[field].apply(log_normalize)\n",
    "        os.makedirs(f'{output_path}/ood_test', exist_ok=True)\n",
    "        df.to_csv(f'{output_path}/ood_test/{type}', index=False)\n",
    "    else:\n",
    "        for file in os.listdir(f'{output_path}/unclean_ood_test/{type}'):\n",
    "            df = pd.read_csv(f'{output_path}/unclean_ood_test/{type}/{file}')\n",
    "            if 'TCP_30' in df.columns:\n",
    "                print('OK')\n",
    "                df = df.drop(columns = ['TCP_30'])\n",
    "            print(type, file)\n",
    "            for field in normalize_set:\n",
    "                if field in df.columns:\n",
    "                    df[field] = df[field].apply(log_normalize)\n",
    "            df.to_csv(f'{output_path}/ood_test/{type}/{file}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For VPN\n",
    "field_set = {'TCP_MSS', 'TCP_seq', 'IP_id', 'IP_chksum', 'TCP_urgptr', 'UDP_len', 'TCP_echo_SAck', 'UDP_chksum', 'TCP_echo_Timestamp', 'IP_version', 'TCP_window', 'IP_len', 'IP_flags', 'TCP_SAck', 'TCP_WScale', 'TCP_EOL', 'TCP_ack', 'TCP_chksum', 'IP_frag', 'TCP_flags', 'IP_ttl', 'IP_ihl', 'TCP_dataofs', 'TCP_Timestamp', 'IP_proto', 'IP_tos', 'TCP_reserved', 'TCP_SAckOK'}\n",
    "\n",
    "ip_flags_map = {'DF': 1}\n",
    "tcp_flags_map = {'R': 1, 'SA': 2, 'RA': 3, 'S': 4, 'PA': 5, 'FPA': 6, 'A': 7, 'FA': 8}\n",
    "ip_proto_map = {'tcp': 1, 'udp': 2}\n",
    "\n",
    " normalize_set = {'TCP_MSS', 'TCP_seq', 'IP_id', 'IP_chksum','UDP_len', 'TCP_echo_SAck', 'UDP_chksum', 'TCP_echo_Timestamp','TCP_window', 'IP_len',  'TCP_SAck','TCP_ack', 'TCP_chksum', 'TCP_Timestamp', 'IP_tos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "normalize_set = {'TL_chksum', 'TCP_MSS', 'TCP_seq', 'IP_id', 'IP_chksum', 'UDP_len', 'TCP_echo_SAck', 'UDP_chksum', 'TCP_echo_Timestamp','TCP_window', 'IP_len',  'TCP_SAck','TCP_ack', 'TCP_chksum', 'TCP_Timestamp', 'IP_tos'}\n",
    "types = ['train_val_split_0', 'train_val_split_1', 'train_val_split_2', 'test.csv']\n",
    "\n",
    "def log_normalize(value):\n",
    "    return np.log1p(value)\n",
    "\n",
    "for type in types:\n",
    "    if type == 'test.csv':\n",
    "        df = pd.read_csv(f'{output_path}/unclean_add_info/{type}')\n",
    "        for field in normalize_set:\n",
    "            if field in df.columns:\n",
    "                df[field] = df[field].apply(log_normalize)\n",
    "        df.to_csv(f'{output_path}/add_info/{type}', index=False)\n",
    "    else:\n",
    "        for file in os.listdir(f'{output_path}/unclean_add_info/{type}'):\n",
    "            df = pd.read_csv(f'{output_path}/unclean_add_info/{type}/{file}')\n",
    "            for field in normalize_set:\n",
    "                if field in df.columns:\n",
    "                    df[field] = df[field].apply(log_normalize)\n",
    "            df.to_csv(f'{output_path}/add_info/{type}/{file}', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_process]",
   "language": "python",
   "name": "conda-env-data_process-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
